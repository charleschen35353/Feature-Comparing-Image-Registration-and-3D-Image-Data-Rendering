{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2.0\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "print(cv2.__version__)\n",
    "print(tf.__version__)\n",
    "assert tf.executing_eagerly() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 620\n",
    "IMG_HEIGHT = 877\n",
    "IMG_CHN = 3\n",
    "NUM_F_POINTS = 5000\n",
    "NUM_MATCHES = 500\n",
    "BBOX_LENGTH = 21    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "NUM_F_POINTS = 2000\n",
    "\n",
    "# Batch processing of cv2 non-learning based method\n",
    "def extract_features(ref_image, sns_image):\n",
    "    # Create ORB detector with 5000 features. \n",
    "\n",
    "    ref_image = cv2.cvtColor(ref_image, cv2.COLOR_BGR2GRAY) \n",
    "    sns_image = cv2.cvtColor(sns_image, cv2.COLOR_BGR2GRAY) \n",
    "    orb_detector = cv2.ORB_create(5000) \n",
    "    kp1, d1 = orb_detector.detectAndCompute(ref_image, None) \n",
    "    kp2, d2 = orb_detector.detectAndCompute(sns_image, None) \n",
    "    return [kp1,kp2,d1,d2]\n",
    "\n",
    "def extract_feature_batch(refs, sns):\n",
    "    output = [[],[],[],[]]\n",
    "    for i in range(refs.shape[0]):\n",
    "        out = extract_features(refs[i], sns[i])\n",
    "        for j in range(4):\n",
    "            output[j].append(out[j])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def batch_match(d1s, d2s):\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    matches = []\n",
    "\n",
    "    for i in range(len(d1s)):\n",
    "        matches.append(matcher.match(d1s[i], d2s[i]))\n",
    "    return matches\n",
    "\n",
    "def validate_match(matches):\n",
    "    for i in range(len(matches)):\n",
    "        matches[i].sort(key = lambda x: x.distance)\n",
    "        matches[i] = matches[i][:int(len(matches)*60)]\n",
    "    return matches\n",
    "\n",
    "def calc_homographies(kp1s, kp2s, matches):\n",
    "    # Define empty matrices of shape no_of_matches * 2. \n",
    "    homographies = []\n",
    "    temp = matches\n",
    "    matches = []\n",
    "\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i] != []:\n",
    "            matches.append(list(filter(None, temp[i])))\n",
    "            \n",
    "    matches = validate_match(matches)\n",
    "    \n",
    "    for i in range(len(matches)):\n",
    "        p1 = np.zeros((len(matches[i]), 2)) \n",
    "        p2 = np.zeros((len(matches[i]), 2)) \n",
    "        if p1.shape[0] != 0:\n",
    "            for j in range(len(matches[i])):\n",
    "                p1[j, :] = kp1s[i][matches[i][j].queryIdx].pt \n",
    "                p2[j, :] = kp2s[i][matches[i][j].trainIdx].pt \n",
    "            homography, _ = cv2.findHomography(p1, p2, cv2.RANSAC) \n",
    "        else:\n",
    "            homography = np.zeros([3,3])\n",
    "        homographies.append(homography)\n",
    "    return homographies\n",
    "\n",
    "def register_images(sns_imgs, homographies, img_size = (IMG_WIDTH,IMG_HEIGHT), save = False):\n",
    "    # Use this matrix to transform the \n",
    "    # colored image wrt the reference image. \n",
    "    transformed_imgs = []\n",
    "    for i in range(sns_imgs.shape[0]):\n",
    "        transformed_img = cv2.warpPerspective(sns_imgs[i], \n",
    "                            homographies[i], img_size) \n",
    "        if save: \n",
    "            cv2.imwrite('aligned_{}.jpg'.format(i), transformed_img) \n",
    "        \n",
    "        transformed_imgs.append(transformed_img)\n",
    "    return transformed_imgs\n",
    "\n",
    "def visualize_matches(ref_imgs, sns_imgs, kp1s, kp2s, matches):\n",
    "    for i in range(ref_imgs.shape[0]):\n",
    "        imMatches = cv2.drawMatches(ref_imgs[i], kp1s[i], sns_imgs[i], kp2s[i], matches[i], None)\n",
    "        cv2.imwrite(\"matches_{}.jpg\".format(i), imMatches)\n",
    "    \n",
    "    \n",
    "def get_alignment_matrix(kprs, kpss, drs, dss):\n",
    "    '''\n",
    "    Inputs\n",
    "        kprs: F keypoints for each reference(query) image of shape N*F*3 with X,Y,Size\n",
    "        kpss: F keypoints for each sensed(train) image of shape N*F*3 with X,Y,Size\n",
    "        drs: F feature descriptors for each reference(query) image of shape N*F*32 \n",
    "        dss: F feature descriptors for each sensed(train) image of shape N*F*32 \n",
    "    Output\n",
    "        aligned feature points and correlated distance of size N*f*7 X1,Y1,Size1, X2, Y2, Size2, Distance\n",
    "    '''\n",
    "\n",
    "    alignment_matrices = None\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    for i in range(kprs.shape[0]):\n",
    "        results = matcher.match(d1s[i], d2s[i])\n",
    "        aligned = []\n",
    "        for r in results:\n",
    "            if len(aligned) >= NUM_MATCHES: break\n",
    "            temp = np.concatenate((kprs[i][r.queryIdx], kpss[i][r.trainIdx]))\n",
    "            aligned.append(np.concatenate((temp, [r.distance])))\n",
    "            \n",
    "        while len(aligned) < NUM_MATCHES:\n",
    "            aligned.append([0,0,0,0,0,0,0])\n",
    "            \n",
    "        aligned = np.array([aligned])\n",
    "        if alignment_matrices is None:\n",
    "            alignment_matrices = aligned\n",
    "        else:\n",
    "            alignment_matrices = np.vstack((alignment_matrices, aligned))\n",
    "            \n",
    "    #sample alignment_matrix\n",
    "    alignment_matrices = np.rint(alignment_matrices).astype(np.uint32)\n",
    "    return alignment_matrices\n",
    "    \n",
    "    \n",
    "def extract_feature_coordinates(ref_image, sns_image):\n",
    "    # Create ORB detector with 5000 features. \n",
    "\n",
    "    ref_image = cv2.cvtColor(ref_image, cv2.COLOR_BGR2GRAY) \n",
    "    sns_image = cv2.cvtColor(sns_image, cv2.COLOR_BGR2GRAY) \n",
    "    orb_detector = cv2.ORB_create(NUM_F_POINTS) \n",
    "    kp1, d1 = orb_detector.detectAndCompute(ref_image, None) \n",
    "    kp2, d2 = orb_detector.detectAndCompute(sns_image, None) \n",
    "    kp1_np, kp2_np = [], []\n",
    "    for i in range(NUM_F_POINTS):\n",
    "        if i < len(kp1):\n",
    "            kp1_np.append([kp1[i].pt[0],kp1[i].pt[1], kp1[i].size ] )\n",
    "        else:\n",
    "            kp1_np.append([0,0,0])\n",
    "            d1 = np.vstack((d1, [np.zeros(32, dtype = np.uint8 )]))\n",
    "            \n",
    "        if i < len(kp2):\n",
    "            kp2_np.append([kp2[i].pt[0],kp2[i].pt[1], kp2[i].size ] )\n",
    "        else:\n",
    "            kp2_np.append([0,0,0])\n",
    "            d2 = np.vstack((d2, [np.zeros(32,dtype = np.uint8 )]))\n",
    "        \n",
    "    kp1_np, kp2_np = np.array(kp1_np) , np.array(kp2_np)\n",
    "\n",
    "    return [kp1_np, kp2_np, d1, d2]\n",
    "\n",
    "def extract_feature_coor_batch(refs, sns):\n",
    "    output = []\n",
    "    for i in range(refs.shape[0]):\n",
    "        out = extract_feature_coordinates(refs[i], sns[i])\n",
    "        for j in range(4):\n",
    "            if len(output) < 4:\n",
    "                output.append(np.expand_dims(out[j], axis=0))\n",
    "            else: \n",
    "                output[j] = np.vstack( (output[j],np.expand_dims(out[j], axis=0)) )\n",
    "\n",
    "    return output\n",
    "    \n",
    "\n",
    "    \n",
    "def get_model_inputs(refs,sns):\n",
    "    p_ref, p_sns, matches, kprs, kpss = get_match_info(refs, sns)\n",
    "    p_ref = (p_ref.astype(np.float32) / 255.0 ).reshape((p_ref.shape[0]*p_ref.shape[1],\\\n",
    "                                                        p_ref.shape[2], p_ref.shape[3], p_ref.shape[4]))\n",
    "    p_sns = (p_sns.astype(np.float32) / 255.0).reshape((p_sns.shape[0]*p_sns.shape[1],\\\n",
    "                                                        p_sns.shape[2], p_sns.shape[3], p_sns.shape[4]))\n",
    "    #matches = matches.reshape(matches.shape[0] * matches.shape[1])\n",
    "    return [p_ref, p_sns, matches, kprs, kpss]\n",
    "\n",
    "def patch_dist(p1,p2):\n",
    "\n",
    "    return np.mean(((p1 - p2)**2)**0.5)\n",
    "\n",
    "def get_central_coor(patch,img):\n",
    "    \n",
    "    W,H = patch.shape[0], patch.shape[1]\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            print(\"{} {}\".format(i,j))\n",
    "            if i+W < img.shape[0] and j+H < img.shape[1] and patch_dist(img[i:i+W, j:j+H, :],patch) < 1:\n",
    "                return (i+W/2, j + H/2)\n",
    "            \n",
    "    print(\"patch does not exist in img.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def patch_dist(p1,p2):\n",
    "\n",
    "    return np.mean(((p1 - p2)**2)**0.5)\n",
    "\n",
    "def get_central_coor(patch,img):\n",
    "    \n",
    "    W,H = patch.shape[0], patch.shape[1]\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            print(\"{} {}\".format(i,j))\n",
    "            if i+W < img.shape[0] and j+H < img.shape[1] and patch_dist(img[i:i+W, j:j+H, :],patch) < 1:\n",
    "                return (i+W/2, j + H/2)\n",
    "            \n",
    "    print(\"patch does not exist in img.\")\n",
    "    return None\n",
    "\n",
    "def visualize_corresponding_patches(p1, p2):\n",
    "    for j in range(50):\n",
    "        vis = (np.concatenate((p1[0][j], p2[0][j]), axis=1))*255\n",
    "        cv2.imwrite(\"patch pair {}.jpg\".format(j), vis)\n",
    "        \n",
    "def visualize_coords(img, c):\n",
    "    for j in range(500):\n",
    "        cv2.circle(img[0], (c[0][j][0], c[0][j][1]) , 1, (0, 0, 255), -1)\n",
    "    cv2.imwrite(\"New feture img.jpg\", img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images belonging to 1 classes.\n",
      "Found 6 images belonging to 1 classes.\n",
      "(1, 500, 20, 20, 6)\n",
      "(1, 500, 20, 20, 3)\n",
      "(1, 500, 20, 20, 3)\n"
     ]
    }
   ],
   "source": [
    "def extract_match_patches(ref_imgs, sns_imgs, kprs, kpss, drs, dss):\n",
    "    '''\n",
    "    output: N * NUM_MATCHES * 2 * PATCH_H * PATCH_W * CHN Example:(6, 500, 2, 6, 6, 3)\n",
    "    '''\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    patches, locations = [], []\n",
    "    mask = np.zeros_like(ref_imgs)\n",
    "    for i in range(kprs.shape[0]):\n",
    "        results = matcher.match(drs[i], dss[i])\n",
    "        patch = []\n",
    "        coors = []\n",
    "        for r in results:\n",
    "            pair = []\n",
    "            coor = []\n",
    "            if len(patch) >= NUM_MATCHES: break\n",
    "            x, y, dis = kprs[i][r.queryIdx].astype(np.uint32)\n",
    "            if  x-(BBOX_LENGTH-1) > 0 and y-(BBOX_LENGTH-1) > 0 \\\n",
    "                and x+(BBOX_LENGTH-1)/2 < IMG_WIDTH and y+(BBOX_LENGTH-1)/2 < IMG_HEIGHT:\n",
    "                pair.append(ref_imgs[i][y-int((BBOX_LENGTH-1)/2):y+int((BBOX_LENGTH-1)/2)\\\n",
    "                                       ,x-int((BBOX_LENGTH-1)/2):x+int((BBOX_LENGTH-1)/2)])\n",
    "                coor.append([y,x])\n",
    "                \n",
    "            x, y, dis = kpss[i][r.trainIdx].astype(np.uint32)\n",
    "            if  x-(BBOX_LENGTH-1) > 0 and y-(BBOX_LENGTH-1) > 0 \\\n",
    "                and x+(BBOX_LENGTH-1)/2 < IMG_WIDTH and y+(BBOX_LENGTH-1)/2 < IMG_HEIGHT:\n",
    "                pair.append(sns_imgs[i][y-int((BBOX_LENGTH-1)/2):y+int((BBOX_LENGTH-1)/2)\\\n",
    "                                       ,x-int((BBOX_LENGTH-1)/2):x+int((BBOX_LENGTH-1)/2)])\n",
    "                coor.append([y,x])\n",
    "                \n",
    "            if len(pair) == 2:\n",
    "                patch.append(np.array(pair))\n",
    "                coors.append(np.array(coor))\n",
    "                mask[i][y-int((BBOX_LENGTH-1)/2):y+int((BBOX_LENGTH-1)/2)\\\n",
    "                                       ,x-int((BBOX_LENGTH-1)/2):x+int((BBOX_LENGTH-1)/2)] = 1\n",
    "\n",
    "        while len(patch) < NUM_MATCHES:\n",
    "            patch.append(np.zeros((2,BBOX_LENGTH-1,BBOX_LENGTH-1, IMG_CHN)))\n",
    "            coors.append(np.array([[0,0],[0,0]]))\n",
    "        patch = np.array(patch)\n",
    "        patches.append(patch)\n",
    "        coors = np.array(coors)\n",
    "        locations.append(coors)\n",
    "\n",
    "    patches = np.array(patches)\n",
    "    locations = np.array(locations)\n",
    "    return [patches[:,:,0,:,:,:], patches[:,:,1,:,:,:], locations[:,:,0,:], locations[:,:,1,:], mask ]\n",
    "\n",
    "\n",
    "def get_match_info(refs,sns):\n",
    "    \"\"\"\n",
    "    returns in 255 scale\n",
    "    \"\"\"\n",
    "    kprs,kpss, drs, dss = extract_feature_coor_batch(refs,sns)\n",
    "    p_ref, p_sns, coor_ref, coor_sns, mask =  extract_match_patches(refs, sns, kprs, kpss, drs, dss)\n",
    "    return [p_ref, p_sns, coor_ref, coor_sns, mask]\n",
    "    \n",
    "def get_model_inputs(refs,sns):\n",
    "\n",
    "    p_ref, p_sns, coor_ref, coor_sns, mask = get_match_info(refs, sns)\n",
    "    p_ref = (p_ref.astype(np.float32) / 255.0 )\n",
    "    p_sns = (p_sns.astype(np.float32) / 255.0 )\n",
    "    coor_ref = (coor_ref.astype(np.float32) )\n",
    "    coor_sns = (coor_sns.astype(np.float32)  )\n",
    "    #matches = matches.reshape(matches.shape[0] * matches.shape[1])\n",
    "    return [p_ref, p_sns, coor_ref, coor_sns, mask]\n",
    "\n",
    "\n",
    "def generate_generator_multiple(generator, path, batch_size = 16, img_height = IMG_HEIGHT, img_width = IMG_WIDTH):\n",
    "\n",
    "        gen_ref = generator.flow_from_directory(path,\n",
    "                                              classes = [\"ref\"],\n",
    "                                              target_size = (img_height,img_width),\n",
    "                                              batch_size = batch_size,\n",
    "                                              shuffle=False, \n",
    "                                              seed=7)\n",
    "\n",
    "        gen_sns = generator.flow_from_directory(path,\n",
    "                                              classes = [\"sns\"],\n",
    "                                              target_size = (img_height,img_width),\n",
    "                                              batch_size = batch_size,\n",
    "                                              shuffle=False, \n",
    "                                              seed=7)\n",
    "        while True:\n",
    "                X1i = gen_ref.next() #in 255 scale\n",
    "                X2i = gen_sns.next()\n",
    "                \n",
    "                pr,ps,cpr,cps,mask = get_model_inputs(X1i[0].astype(np.uint8), X2i[0].astype(np.uint8))\n",
    "                # x,y in shape of batch_size * NUM_MATCHES* BOX_H * BOX_W * CHN\n",
    "                # cx,cy in shape of batch_size * NUM_MATCHES * 2(x,y)\n",
    "                patch_input = np.concatenate((pr,ps), axis = 4)\n",
    "                #imgs = np.concatenate((X1i[0], X2i[0]), axis = 3)\n",
    "                coors = np.concatenate((cpr,cps), axis = -1)\n",
    "                cv2.imwrite(\"mask.jpg\", mask[0]*255)\n",
    "                \n",
    "                cv2.imwrite(\"masked_ref.jpg\",X1i[0][0]*mask[0])\n",
    "                yield [patch_input, coors, mask, X1i[0], X2i[0]], X1i[0] #Yield both images and their mutual label\n",
    "\n",
    "class Dataloader:\n",
    "    def __init__(self, train_path, test_path, batch_size = 16):\n",
    "        \n",
    "        train_imgen = keras.preprocessing.image.ImageDataGenerator()\n",
    "        test_imgen = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "        self.train_generator = generate_generator_multiple(generator=train_imgen,\n",
    "                                               path = str(train_path),\n",
    "                                               batch_size=batch_size)       \n",
    "\n",
    "        self.test_generator = generate_generator_multiple(test_imgen,\n",
    "                                              path = str(test_path),\n",
    "                                              batch_size=batch_size)              \n",
    "\n",
    "        \n",
    "    def load_data(self, test = False):\n",
    "        if test:\n",
    "            return next(self.test_generator)\n",
    "        else:\n",
    "            return next(self.train_generator)\n",
    "    \n",
    "    def load_dl(self):\n",
    "        return [self.train_generator, self.test_generator]\n",
    "\n",
    "dl = Dataloader(train_path = \"./data/train\", test_path= \"./data/test\", batch_size = 1)\n",
    "x, y = dl.load_data()\n",
    "ps = x[0]\n",
    "print(ps.shape)\n",
    "p1 = x[0][:,:,:,:,:3]\n",
    "p2 = x[0][:,:,:,:,3:]\n",
    "print(p1.shape)\n",
    "print(p2.shape)\n",
    "visualize_corresponding_patches(p1,p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DIST_THRES = 100\n",
    "RANDOM_PROB = 0.15\n",
    "eps = 0.9\n",
    "lr = 5e-4\n",
    "\n",
    "def registration_loss(mask, ref_img, sns_img):\n",
    "    def inner_loss(y_pred, y_true):\n",
    "        homo = y_pred\n",
    "        algined = tf.nn.convolution(sns_img, homo, padding = 'SAME')\n",
    "        masked_aligned = sns_img * mask\n",
    "        masked_ref = ref_img * mask\n",
    "        #p_aligned = VGG_bc4.predict(preprocess_input(masked_algined))\n",
    "        #p_ref = VGG_bc4.predict(preprocess_input(masked_ref))\n",
    "        #loss = tf.reduce_sum(tf.reduce_mean((p_aligned - p_ref)**2))\n",
    "        loss = tf.reduce_sum(tf.reduce_mean((masked_aligned - masked_ref)**2))\n",
    "        return loss\n",
    "    \n",
    "    return inner_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureAlignNet(): \n",
    "    \n",
    "    #similarity value for each match\n",
    "    patches_plh = layers.Input(shape = (NUM_MATCHES, BBOX_LENGTH-1, BBOX_LENGTH-1, IMG_CHN*2),\\\n",
    "                               dtype = 'float32', name = \"input_patches\" )\n",
    "    \n",
    "    p = layers.Reshape((BBOX_LENGTH-1, BBOX_LENGTH-1, IMG_CHN*2*NUM_MATCHES))(patches_plh)\n",
    "    p = layers.Conv2D(16, 3, strides=(2, 2), name = \"pr_conv1\",\\\n",
    "                                padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')(p)\n",
    "    p = layers.Conv2D(64, 3, strides=(2, 2), name = \"pr_conv2\",\\\n",
    "                                padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')(p)\n",
    "    p = layers.Conv2D(128, 3, strides=(1, 1), name = \"pr_conv3\",\\\n",
    "                                padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')(p)\n",
    "    p = layers.MaxPool2D()(p)\n",
    "    p = layers.Flatten()(p)\n",
    "    p = layers.Dense(16*NUM_MATCHES, activation= 'relu', name = \"pr_fc1\", kernel_initializer = 'glorot_uniform')(p)\n",
    "    w = layers.Dense(NUM_MATCHES, activation= 'relu', name = \"pr_out\", kernel_initializer = 'glorot_uniform')(p)\n",
    "    \n",
    "    #homography calc\n",
    "    coors_plh = layers.Input(shape = (NUM_MATCHES, 4), dtype = 'float32', name = \"input_coors\" )\n",
    "    c = layers.Flatten()(coors_plh)\n",
    "    cw = layers.Concatenate(axis=-1)([c,w])\n",
    "    #h = layers.Dense(256, activation= 'relu', name = \"homo_fc1\", kernel_initializer = 'glorot_uniform')(cw)\n",
    "    h = layers.Dense(81, activation= 'relu', name = \"homo_out\", kernel_initializer = 'glorot_uniform')(cw)\n",
    "    homo_p = layers.Reshape((3,3,3,3))(h)\n",
    "    \n",
    "    mask_plh = keras.Input(shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHN),\\\n",
    "                               dtype = 'float32', name = \"input_mask\" )\n",
    "    ref_plh = keras.Input(shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHN),\\\n",
    "                               dtype = 'float32', name = \"input_ref\" )\n",
    "    sns_plh = keras.Input(shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHN),\\\n",
    "                               dtype = 'float32', name = \"input_sns\" )\n",
    "    \n",
    "    model = keras.Model(inputs=[patches_plh, coors_plh, mask_plh, ref_plh, sns_plh], outputs=homo)\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss=registration_loss(mask_plh, ref_plh, sns_plh),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_patches (InputLayer)      [(None, 500, 20, 20, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_59 (Reshape)            (None, 20, 20, 3000) 0           input_patches[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pr_conv1 (Conv2D)               (None, 9, 9, 16)     432016      reshape_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pr_conv2 (Conv2D)               (None, 4, 4, 64)     9280        pr_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pr_conv3 (Conv2D)               (None, 2, 2, 128)    73856       pr_conv2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling2D) (None, 1, 1, 128)    0           pr_conv3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_62 (Flatten)            (None, 128)          0           max_pooling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_coors (InputLayer)        [(None, 500, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pr_fc1 (Dense)                  (None, 8000)         1032000     flatten_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_63 (Flatten)            (None, 2000)         0           input_coors[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pr_out (Dense)                  (None, 500)          4000500     pr_fc1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 2500)         0           flatten_63[0][0]                 \n",
      "                                                                 pr_out[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "homo_fc1 (Dense)                (None, 128)          320128      concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "homo_out (Dense)                (None, 27)           3483        homo_fc1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 877, 620, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ref (InputLayer)          [(None, 877, 620, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_sns (InputLayer)          [(None, 877, 620, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_60 (Reshape)            (None, 3, 3, 3)      0           homo_out[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,871,263\n",
      "Trainable params: 5,871,263\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-391:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 874, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 866, in pool_fn\n",
      "    initargs=(seqs, self.random_seed, get_worker_id_queue()))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 174, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 239, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 66, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-8683277e412a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                 \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestset_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                 \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                 shuffle = True)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "trainset_size = 6\n",
    "testset_size = 6\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "dl = Dataloader(train_path = \"./data/train\", test_path= \"./data/test\", batch_size = batch_size)\n",
    "train_generator, test_generator = dl.load_dl()\n",
    "net = FeatureAlignNet()\n",
    "net.fit_generator(train_generator,\n",
    "                                steps_per_epoch=trainset_size/batch_size,\n",
    "                                epochs = epochs,\n",
    "                                validation_data = test_generator,\n",
    "                                validation_steps = testset_size/batch_size,\n",
    "                                use_multiprocessing = True,\n",
    "                                shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches, additionals = dl.load_data(test=True)\n",
    "pred = net(patches)\n",
    "refs,snss,matches = additionals\n",
    "imgs = register_with_predicted(pred, matches, refs, snss)\n",
    "for i in range(imgs.shape[0]):\n",
    "    cv2.imwrite(\"registered_cv2{}.jpg\".format(i), imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-16547cebd750>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-16547cebd750>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    net.compile(loss = registration_loss(out, imgs_inputs), optimizer = , run_eagerly = True)\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "def patch_ranker():\n",
    "    \n",
    "    patch_inputs = layers.Input(shape = (20, 20, 6))\n",
    "    imgs_inputs = layers.Input(shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHN))\n",
    "\n",
    "    x = layers.Conv2D(16, 3 ,strides=(2, 2), name = \"conv1\",\\\n",
    "                                    padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')(patch_inputs)\n",
    "    x = layers.Conv2D(64, 3, strides=(1, 1), name = \"conv2\",\\\n",
    "                                    padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')(x)\n",
    "    x = layers.Conv2D(128, 3, strides=(1, 1), name = \"conv3\",\\\n",
    "                                    padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(32, activation= 'relu', name = \"fc1\", kernel_initializer = 'glorot_uniform')(x)\n",
    "    out = layers.Dense(1, activation= 'sigmoid', name = \"fc2\", kernel_initializer = 'glorot_uniform')(x)\n",
    "\n",
    "    net = keras.Model(inputs = [patch_inputs, imgs_inputs], outputs = out)\n",
    "\n",
    "    net.compile(loss = registration_loss(out, imgs_inputs), optimizer = , run_eagerly = True)\n",
    "\n",
    "    return net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
