{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.2\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "print(cv2.__version__)\n",
    "print(tf.__version__)\n",
    "assert tf.executing_eagerly() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 620\n",
    "IMG_HEIGHT = 877\n",
    "IMG_CHN = 3\n",
    "NUM_F_POINTS = 5000\n",
    "NUM_MATCHES = 500\n",
    "BBOX_LENGTH = 21    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "NUM_F_POINTS = 2000\n",
    "\n",
    "# Batch processing of cv2 non-learning based method\n",
    "def extract_features(ref_image, sns_image):\n",
    "    # Create ORB detector with 5000 features. \n",
    "\n",
    "    ref_image = cv2.cvtColor(ref_image, cv2.COLOR_BGR2GRAY) \n",
    "    sns_image = cv2.cvtColor(sns_image, cv2.COLOR_BGR2GRAY) \n",
    "    orb_detector = cv2.ORB_create(5000) \n",
    "    kp1, d1 = orb_detector.detectAndCompute(ref_image, None) \n",
    "    kp2, d2 = orb_detector.detectAndCompute(sns_image, None) \n",
    "    return [kp1,kp2,d1,d2]\n",
    "\n",
    "def extract_feature_batch(refs, sns):\n",
    "    output = [[],[],[],[]]\n",
    "    for i in range(refs.shape[0]):\n",
    "        out = extract_features(refs[i], sns[i])\n",
    "        for j in range(4):\n",
    "            output[j].append(out[j])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def batch_match(d1s, d2s):\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    matches = []\n",
    "\n",
    "    for i in range(len(d1s)):\n",
    "        matches.append(matcher.match(d1s[i], d2s[i]))\n",
    "    return matches\n",
    "\n",
    "def validate_match(matches):\n",
    "    for i in range(len(matches)):\n",
    "        matches[i].sort(key = lambda x: x.distance)\n",
    "        matches[i] = matches[i][:int(len(matches)*60)]\n",
    "    return matches\n",
    "\n",
    "def calc_homographies(kp1s, kp2s, matches):\n",
    "    # Define empty matrices of shape no_of_matches * 2. \n",
    "    homographies = []\n",
    "    temp = matches\n",
    "    matches = []\n",
    "\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i] != []:\n",
    "            matches.append(list(filter(None, temp[i])))\n",
    "            \n",
    "    matches = validate_match(matches)\n",
    "    \n",
    "    for i in range(len(matches)):\n",
    "        p1 = np.zeros((len(matches[i]), 2)) \n",
    "        p2 = np.zeros((len(matches[i]), 2)) \n",
    "        if p1.shape[0] != 0:\n",
    "            for j in range(len(matches[i])):\n",
    "                p1[j, :] = kp1s[i][matches[i][j].queryIdx].pt \n",
    "                p2[j, :] = kp2s[i][matches[i][j].trainIdx].pt \n",
    "            homography, _ = cv2.findHomography(p1, p2, cv2.RANSAC) \n",
    "        else:\n",
    "            homography = np.zeros([3,3])\n",
    "        homographies.append(homography)\n",
    "    return homographies\n",
    "\n",
    "def register_images(sns_imgs, homographies, img_size = (IMG_WIDTH,IMG_HEIGHT), save = False):\n",
    "    # Use this matrix to transform the \n",
    "    # colored image wrt the reference image. \n",
    "    transformed_imgs = []\n",
    "    for i in range(sns_imgs.shape[0]):\n",
    "        transformed_img = cv2.warpPerspective(sns_imgs[i], \n",
    "                            homographies[i], img_size) \n",
    "        if save: \n",
    "            cv2.imwrite('aligned_{}.jpg'.format(i), transformed_img) \n",
    "        \n",
    "        transformed_imgs.append(transformed_img)\n",
    "    return transformed_imgs\n",
    "\n",
    "def visualize_matches(ref_imgs, sns_imgs, kp1s, kp2s, matches):\n",
    "    for i in range(ref_imgs.shape[0]):\n",
    "        imMatches = cv2.drawMatches(ref_imgs[i], kp1s[i], sns_imgs[i], kp2s[i], matches[i], None)\n",
    "        cv2.imwrite(\"matches_{}.jpg\".format(i), imMatches)\n",
    "    \n",
    "    \n",
    "def get_alignment_matrix(kprs, kpss, drs, dss):\n",
    "    '''\n",
    "    Inputs\n",
    "        kprs: F keypoints for each reference(query) image of shape N*F*3 with X,Y,Size\n",
    "        kpss: F keypoints for each sensed(train) image of shape N*F*3 with X,Y,Size\n",
    "        drs: F feature descriptors for each reference(query) image of shape N*F*32 \n",
    "        dss: F feature descriptors for each sensed(train) image of shape N*F*32 \n",
    "    Output\n",
    "        aligned feature points and correlated distance of size N*f*7 X1,Y1,Size1, X2, Y2, Size2, Distance\n",
    "    '''\n",
    "\n",
    "    alignment_matrices = None\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    for i in range(kprs.shape[0]):\n",
    "        results = matcher.match(d1s[i], d2s[i])\n",
    "        aligned = []\n",
    "        for r in results:\n",
    "            if len(aligned) >= NUM_MATCHES: break\n",
    "            temp = np.concatenate((kprs[i][r.queryIdx], kpss[i][r.trainIdx]))\n",
    "            aligned.append(np.concatenate((temp, [r.distance])))\n",
    "            \n",
    "        while len(aligned) < NUM_MATCHES:\n",
    "            aligned.append([0,0,0,0,0,0,0])\n",
    "            \n",
    "        aligned = np.array([aligned])\n",
    "        if alignment_matrices is None:\n",
    "            alignment_matrices = aligned\n",
    "        else:\n",
    "            alignment_matrices = np.vstack((alignment_matrices, aligned))\n",
    "            \n",
    "    #sample alignment_matrix\n",
    "    alignment_matrices = np.rint(alignment_matrices).astype(np.uint32)\n",
    "    return alignment_matrices\n",
    "    \n",
    "    \n",
    "def test_cv2_batch(dl):\n",
    "    refs, sns = dl.load_image()\n",
    "    kp1s, kp2s, d1s,d2s = extract_feature_batch(refs,sns)\n",
    "    matches = batch_match(d1s, d2s)\n",
    "    \n",
    "    visualize_matches(refs, sns, kp1s, kp2s, matches)\n",
    "    homos = calc_homographies(kp1s, kp2s, matches)\n",
    "    imgs = register_images(sns, homos)\n",
    "    for i in range(6):\n",
    "            cv2.imwrite(\"registered_cv2{}.jpg\".format(i), imgs[i])\n",
    "        \n",
    "\n",
    "    \n",
    "def extract_feature_coordinates(ref_image, sns_image):\n",
    "    # Create ORB detector with 5000 features. \n",
    "\n",
    "    ref_image = cv2.cvtColor(ref_image, cv2.COLOR_BGR2GRAY) \n",
    "    sns_image = cv2.cvtColor(sns_image, cv2.COLOR_BGR2GRAY) \n",
    "    orb_detector = cv2.ORB_create(NUM_F_POINTS) \n",
    "    kp1, d1 = orb_detector.detectAndCompute(ref_image, None) \n",
    "    kp2, d2 = orb_detector.detectAndCompute(sns_image, None) \n",
    "    kp1_np, kp2_np = [], []\n",
    "    for i in range(NUM_F_POINTS):\n",
    "        if i < len(kp1):\n",
    "            kp1_np.append([kp1[i].pt[0],kp1[i].pt[1], kp1[i].size ] )\n",
    "        else:\n",
    "            kp1_np.append([0,0,0])\n",
    "            d1 = np.vstack((d1, [np.zeros(32, dtype = np.uint8 )]))\n",
    "            \n",
    "        if i < len(kp2):\n",
    "            kp2_np.append([kp2[i].pt[0],kp2[i].pt[1], kp2[i].size ] )\n",
    "        else:\n",
    "            kp2_np.append([0,0,0])\n",
    "            d2 = np.vstack((d2, [np.zeros(32,dtype = np.uint8 )]))\n",
    "        \n",
    "    kp1_np, kp2_np = np.array(kp1_np) , np.array(kp2_np)\n",
    "\n",
    "    return [kp1_np, kp2_np, d1, d2]\n",
    "\n",
    "def extract_feature_coor_batch(refs, sns):\n",
    "    output = []\n",
    "    for i in range(refs.shape[0]):\n",
    "        out = extract_feature_coordinates(refs[i], sns[i])\n",
    "        for j in range(4):\n",
    "            if len(output) < 4:\n",
    "                output.append(np.expand_dims(out[j], axis=0))\n",
    "            else: \n",
    "                output[j] = np.vstack( (output[j],np.expand_dims(out[j], axis=0)) )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def extract_match_patches(ref_imgs, sns_imgs, kprs, kpss, drs, dss):\n",
    "    '''\n",
    "    output: N * NUM_MATCHES * 2 * PATCH_H * PATCH_W * CHN Example:(6, 500, 2, 6, 6, 3)\n",
    "    '''\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    patches, matches = [], []\n",
    "    for i in range(kprs.shape[0]):\n",
    "        results = matcher.match(drs[i], dss[i])\n",
    "        patch = []\n",
    "        match = []\n",
    "        for r in results:\n",
    "            pair = []\n",
    "            match_p = []\n",
    "            if len(patch) >= NUM_MATCHES: break\n",
    "            x, y, dis = kprs[i][r.queryIdx].astype(np.uint32)\n",
    "            if  x-(BBOX_LENGTH-1) > 0 and y-(BBOX_LENGTH-1) > 0 \\\n",
    "                and x+(BBOX_LENGTH-1)/2 < IMG_WIDTH and y+(BBOX_LENGTH-1)/2 < IMG_HEIGHT:\n",
    "                pair.append(ref_imgs[i][y-int((BBOX_LENGTH-1)/2):y+int((BBOX_LENGTH-1)/2)\\\n",
    "                                       ,x-int((BBOX_LENGTH-1)/2):x+int((BBOX_LENGTH-1)/2)])\n",
    "                \n",
    "            x, y, dis = kpss[i][r.trainIdx].astype(np.uint32)\n",
    "            if  x-(BBOX_LENGTH-1) > 0 and y-(BBOX_LENGTH-1) > 0 \\\n",
    "                and x+(BBOX_LENGTH-1)/2 < IMG_WIDTH and y+(BBOX_LENGTH-1)/2 < IMG_HEIGHT:\n",
    "                pair.append(sns_imgs[i][y-int((BBOX_LENGTH-1)/2):y+int((BBOX_LENGTH-1)/2)\\\n",
    "                                       ,x-int((BBOX_LENGTH-1)/2):x+int((BBOX_LENGTH-1)/2)])\n",
    "                \n",
    "            if len(pair) == 2:\n",
    "                patch.append(np.array(pair))\n",
    "                match.append(r)\n",
    "                \n",
    "        while len(patch) < NUM_MATCHES:\n",
    "            patch.append(np.zeros((2,BBOX_LENGTH-1,BBOX_LENGTH-1, IMG_CHN)))\n",
    "            match.append(None)\n",
    "            \n",
    "        patch = np.array(patch)\n",
    "        patches.append(patch)\n",
    "        match = np.array(match)\n",
    "        matches.append(match)\n",
    "\n",
    "    patches = np.array(patches)\n",
    "    matches = np.array(matches)   \n",
    "\n",
    "    return [patches[:,:,0,:,:,:], patches[:,:,1,:,:,:], matches]\n",
    "    \n",
    "\n",
    "def get_match_info(refs,sns):\n",
    "    \"\"\"\n",
    "    returns in 255 scale\n",
    "    \"\"\"\n",
    "    kprs,kpss, drs, dss = extract_feature_coor_batch(refs,sns)\n",
    "    p_ref, p_sns, matches =  extract_match_patches(refs, sns, kprs, kpss, drs, dss)\n",
    "    return [p_ref, p_sns, matches, kprs, kpss]\n",
    "    \n",
    "    \n",
    "def get_model_inputs(refs,sns):\n",
    "    p_ref, p_sns, matches, kprs, kpss = get_match_info(refs, sns)\n",
    "    p_ref = (p_ref.astype(np.float32) / 255.0 ).reshape((p_ref.shape[0]*p_ref.shape[1],\\\n",
    "                                                        p_ref.shape[2], p_ref.shape[3], p_ref.shape[4]))\n",
    "    p_sns = (p_sns.astype(np.float32) / 255.0).reshape((p_sns.shape[0]*p_sns.shape[1],\\\n",
    "                                                        p_sns.shape[2], p_sns.shape[3], p_sns.shape[4]))\n",
    "    #matches = matches.reshape(matches.shape[0] * matches.shape[1])\n",
    "    return [p_ref, p_sns, matches, kprs, kpss]\n",
    "        \n",
    "def visualize_corresponding_patches(p1, p2):\n",
    "    for j in range(50):\n",
    "        vis = (np.concatenate((p1[0][j], p2[0][j]), axis=1))\n",
    "        cv2.imwrite(\"patch pair {}.jpg\".format(j), vis)\n",
    "        \n",
    "def visualize_coords(img, c):\n",
    "    for j in range(500):\n",
    "        cv2.circle(img[0], (c[0][j][0], c[0][j][1]) , 1, (0, 0, 255), -1)\n",
    "    cv2.imwrite(\"New feture img.jpg\", img[0])\n",
    "\n",
    "\n",
    "def patch_dist(p1,p2):\n",
    "\n",
    "    return np.mean(((p1 - p2)**2)**0.5)\n",
    "\n",
    "def get_central_coor(patch,img):\n",
    "    \n",
    "    W,H = patch.shape[0], patch.shape[1]\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            print(\"{} {}\".format(i,j))\n",
    "            if i+W < img.shape[0] and j+H < img.shape[1] and patch_dist(img[i:i+W, j:j+H, :],patch) < 1:\n",
    "                return (i+W/2, j + H/2)\n",
    "            \n",
    "    print(\"patch does not exist in img.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def patch_dist(p1,p2):\n",
    "\n",
    "    return np.mean(((p1 - p2)**2)**0.5)\n",
    "\n",
    "def get_central_coor(patch,img):\n",
    "    \n",
    "    W,H = patch.shape[0], patch.shape[1]\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            print(\"{} {}\".format(i,j))\n",
    "            if i+W < img.shape[0] and j+H < img.shape[1] and patch_dist(img[i:i+W, j:j+H, :],patch) < 1:\n",
    "                return (i+W/2, j + H/2)\n",
    "            \n",
    "    print(\"patch does not exist in img.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs(refs,sns):\n",
    "    p_ref, p_sns, matches, kprs, kpss = get_match_info(refs, sns)\n",
    "    p_ref = (p_ref.astype(np.float32) / 255.0 ).reshape((p_ref.shape[0]*p_ref.shape[1],\\\n",
    "                                                        p_ref.shape[2], p_ref.shape[3], p_ref.shape[4]))\n",
    "    p_sns = (p_sns.astype(np.float32) / 255.0).reshape((p_sns.shape[0]*p_sns.shape[1],\\\n",
    "                                                        p_sns.shape[2], p_sns.shape[3], p_sns.shape[4]))\n",
    "    #matches = matches.reshape(matches.shape[0] * matches.shape[1])\n",
    "    return [p_ref, p_sns, matches, kprs, kpss]\n",
    "\n",
    "\n",
    "def generate_generator_multiple(generator, path, batch_size = 16, img_height = IMG_HEIGHT, img_width = IMG_WIDTH):\n",
    "\n",
    "        gen_ref = generator.flow_from_directory(path,\n",
    "                                              classes = [\"ref\"],\n",
    "                                              target_size = (img_height,img_width),\n",
    "                                              batch_size = batch_size,\n",
    "                                              shuffle=False, \n",
    "                                              seed=7)\n",
    "\n",
    "        gen_sns = generator.flow_from_directory(path,\n",
    "                                              classes = [\"sns\"],\n",
    "                                              target_size = (img_height,img_width),\n",
    "                                              batch_size = batch_size,\n",
    "                                              shuffle=False, \n",
    "                                              seed=7)\n",
    "        while True:\n",
    "                X1i = gen_ref.next()\n",
    "                X2i = gen_sns.next()\n",
    "                \n",
    "                x,y,matches, _, _ = get_model_inputs(X1i[0].astype(np.uint8), X2i[0].astype(np.uint8))\n",
    "                patch_input = np.concatenate((x,y), axis = 3)\n",
    "                imgs = np.concatenate((X1i[0].astype(np.uint8), X2i[0].astype(np.uint8)), axis = 3)\n",
    "                yield [patch_input,imgs], None #Yield both images and their mutual label\n",
    "\n",
    "class Dataloader:\n",
    "    def __init__(self, train_path, test_path, batch_size = 16):\n",
    "        \n",
    "        train_imgen = keras.preprocessing.image.ImageDataGenerator()\n",
    "        test_imgen = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "        self.train_generator = generate_generator_multiple(generator=train_imgen,\n",
    "                                               path = str(train_path),\n",
    "                                               batch_size=batch_size)       \n",
    "\n",
    "        self.test_generator = generate_generator_multiple(test_imgen,\n",
    "                                              path = str(test_path),\n",
    "                                              batch_size=batch_size)              \n",
    "\n",
    "        \n",
    "    def load_data(self, test = False):\n",
    "        if test:\n",
    "            return next(self.test_generator)\n",
    "        else:\n",
    "            return next(self.train_generator)\n",
    "    \n",
    "    def load_dl(self):\n",
    "        return [self.train_generator, self.test_generator]\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DIST_THRES = 100\n",
    "RANDOM_PROB = 0.15\n",
    "eps = 0.9\n",
    "lr = 5e-4\n",
    "\n",
    "\n",
    "def register_with_predicted(pred, matches, refs, snss):\n",
    "    inds = np.argsort(pred, axis = 1)\n",
    "    \n",
    "    if np.random.uniform() < RANDOM_PROB * eps:#**(self.iteration / 100):\n",
    "        inds = np.random.shuffle(inds)\n",
    "    \n",
    "    mask = np.where(inds < 50, np.ones_like(inds), np.zeros_like(inds)) # -1*NUM_MATCHES\n",
    "    selected_matches = np.where( mask >= 0.5, matches, np.zeros_like(matches)  )\n",
    "    good_matches = []\n",
    "    \n",
    "    for i in range(selected_matches.shape[0]):\n",
    "        good_matches.append(selected_matches[i][selected_matches[i] != 0])\n",
    "\n",
    "    kprs, kpss, _, _ =  extract_feature_batch(refs,snss)\n",
    "    homo = calc_homographies(kprs, kpss, good_matches)\n",
    "    imgs = np.array(register_images(snss, homo))\n",
    "    \n",
    "    return imgs, mask\n",
    "\n",
    "def registration_loss(y_pred, imgs):\n",
    "\n",
    "    \n",
    "    ##############numpy operation###############\n",
    "    refs, snss = imgs[:,:,:,:3], imgs[:,:,:,3:]\n",
    "    _, _, matches, _, _ = get_model_inputs(refs, snss)\n",
    "    \n",
    "\n",
    "    pred = y_pred.numpy()\n",
    "    imgs, mask = register_with_predicted(pred, matches, refs, snss)\n",
    "\n",
    "    _, _, new_matches, kp1s, kp2s = get_match_info(refs, imgs)\n",
    "\n",
    "    feature_diss,coor_diss, gt = [], [], []\n",
    "    \n",
    "    for i in range(kp1s.shape[0]):\n",
    "        coor_dis, feature_dis, valid_count = 0, 0, 0\n",
    "        for r in new_matches[i]:\n",
    "            if r is None:\n",
    "                continue\n",
    "            valid_count+=1\n",
    "            x1, y1, _ = kp1s[i][r.queryIdx]\n",
    "            x2, y2, _ = kp1s[i][r.trainIdx]\n",
    "            coor_dis += ((x1-x2)**2 + (y1-y2)**2)**0.5 #euc dist\n",
    "            feature_dis += r.distance\n",
    "            \n",
    "        coor_diss.append(coor_dis/valid_count)\n",
    "        feature_diss.append(feature_dis/valid_count)\n",
    "\n",
    "    coor_diss = np.array(coor_diss)\n",
    "    feature_diss = np.array(feature_diss)\n",
    "\n",
    "    for c_d, f_d in zip(coor_diss, feature_diss):\n",
    "        gt.append(np.ones_like(pred[0]) * 1/(c_d+f_d) )\n",
    "    gt = np.array(gt)\n",
    "    #################### END ####################\n",
    "    #ground truth here: gt\n",
    "    gt = tf.convert_to_tensor(gt)\n",
    "    loss = tf.reduce_mean(((gt - y_pred)*mask)**2)\n",
    "    \n",
    "    return loss\n",
    "    #def MSE(y_pred, y_true):\n",
    "    #    loss = tf.reduce_mean(((gt - y_pred)*mask)**2)\n",
    "    #    return loss\n",
    "    \n",
    "    #return MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchRankingNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, dynamic=True):\n",
    "        super(PatchRankingNet, self).__init__(dynamic=dynamic)\n",
    "        self.conv1 = layers.Conv2D(16, 3 ,strides=(2, 2), name = \"conv1\",\\\n",
    "                                padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')\n",
    "        self.conv2 = layers.Conv2D(64, 3, strides=(1, 1), name = \"conv2\",\\\n",
    "                                padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')\n",
    "        self.conv3 = layers.Conv2D(128, 3, strides=(1, 1), name = \"conv3\",\\\n",
    "                                padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')\n",
    "        self.flat = layers.Flatten()\n",
    "        self.fc1 = layers.Dense(32, activation= 'relu', name = \"fc1\", kernel_initializer = 'glorot_uniform')\n",
    "        self.fc2 = layers.Dense(1, activation= 'sigmoid', name = \"fc2\", kernel_initializer = 'glorot_uniform')\n",
    "        \n",
    "    def call(self, inputs, training=True, mask=None):\n",
    "        x, imgs = inputs\n",
    "        imgs = imgs.numpy()\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.reshape(x, (-1, NUM_MATCHES))\n",
    "        self.add_loss(registration_loss(x,imgs))\n",
    "        return x\n",
    "\n",
    "\n",
    "#create DNN model\n",
    "\n",
    "\n",
    "net = PatchRankingNet(dynamic = True)\n",
    "net.compile(loss = None, optimizer =  keras.optimizers.Adam(learning_rate=lr), run_eagerly = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "Found 6 images belonging to 1 classes.\n",
      "Found 6 images belonging to 1 classes.\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Output output_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to output_1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "WARNING:tensorflow:From /home/charles/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Found 6 images belonging to 1 classes.\n",
      "Found 6 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.0222 - val_loss: 0.0131\n",
      "Epoch 2/1000\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n",
      "(50, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 874, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 866, in pool_fn\n",
      "    initargs=(seqs, self.random_seed, get_worker_id_queue()))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 174, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 239, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 66, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n",
      "Process Keras_worker_ForkPoolWorker-3:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-291d508deedb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                 \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestset_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                 \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                 shuffle = True)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 828, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"<ipython-input-12-c8b11e7ec3cc>\", line 27, in generate_generator_multiple\n",
      "    X1i = gen_ref.next()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
      "    interpolation=self.interpolation)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
      "    img = img.resize(width_height_tuple, resample)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/Image.py\", line 1817, in resize\n",
      "    self.load()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/ImageFile.py\", line 242, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "Process Keras_worker_ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 828, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"<ipython-input-12-c8b11e7ec3cc>\", line 28, in generate_generator_multiple\n",
      "    X2i = gen_sns.next()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
      "    interpolation=self.interpolation)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
      "    img = img.resize(width_height_tuple, resample)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/Image.py\", line 1817, in resize\n",
      "    self.load()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/ImageFile.py\", line 242, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "trainset_size = 6\n",
    "testset_size = 6\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "dl = Dataloader(train_path = \"./data/train\", test_path= \"./data/test\", batch_size = batch_size)\n",
    "train_generator, test_generator = dl.load_dl()\n",
    "#net = patch_ranker()\n",
    "net.fit_generator(train_generator,\n",
    "                                steps_per_epoch=trainset_size/batch_size,\n",
    "                                epochs = epochs,\n",
    "                                validation_data = test_generator,\n",
    "                                validation_steps = testset_size/batch_size,\n",
    "                                use_multiprocessing = True,\n",
    "                                shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches, additionals = dl.load_data(test=True)\n",
    "pred = net(patches)\n",
    "refs,snss,matches = additionals\n",
    "imgs = register_with_predicted(pred, matches, refs, snss)\n",
    "for i in range(imgs.shape[0]):\n",
    "    cv2.imwrite(\"registered_cv2{}.jpg\".format(i), imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-16547cebd750>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-16547cebd750>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    net.compile(loss = registration_loss(out, imgs_inputs), optimizer = , run_eagerly = True)\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "def patch_ranker():\n",
    "    \n",
    "    patch_inputs = layers.Input(shape = (20, 20, 6))\n",
    "    imgs_inputs = layers.Input(shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHN))\n",
    "\n",
    "    x = layers.Conv2D(16, 3 ,strides=(2, 2), name = \"conv1\",\\\n",
    "                                    padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')(patch_inputs)\n",
    "    x = layers.Conv2D(64, 3, strides=(1, 1), name = \"conv2\",\\\n",
    "                                    padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')(x)\n",
    "    x = layers.Conv2D(128, 3, strides=(1, 1), name = \"conv3\",\\\n",
    "                                    padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(32, activation= 'relu', name = \"fc1\", kernel_initializer = 'glorot_uniform')(x)\n",
    "    out = layers.Dense(1, activation= 'sigmoid', name = \"fc2\", kernel_initializer = 'glorot_uniform')(x)\n",
    "\n",
    "    net = keras.Model(inputs = [patch_inputs, imgs_inputs], outputs = out)\n",
    "\n",
    "    net.compile(loss = registration_loss(out, imgs_inputs), optimizer = , run_eagerly = True)\n",
    "\n",
    "    return net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
