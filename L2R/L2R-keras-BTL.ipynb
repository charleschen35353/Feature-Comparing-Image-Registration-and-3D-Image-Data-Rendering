{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/charles/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.2\n",
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "print(cv2.__version__)\n",
    "print(tf.__version__)\n",
    "assert tf.executing_eagerly() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 620\n",
    "IMG_HEIGHT = 877\n",
    "IMG_CHN = 3\n",
    "NUM_F_POINTS = 5000\n",
    "NUM_MATCHES = 500\n",
    "BBOX_LENGTH = 21    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Batch processing of cv2 non-learning based method\n",
    "print(tf.executing_eagerly())\n",
    "def extract_features(ref_image, sns_image):\n",
    "    # Create ORB detector with 5000 features. \n",
    "\n",
    "    ref_image = cv2.cvtColor(ref_image, cv2.COLOR_BGR2GRAY) \n",
    "    sns_image = cv2.cvtColor(sns_image, cv2.COLOR_BGR2GRAY) \n",
    "    orb_detector = cv2.ORB_create(5000) \n",
    "    kp1, d1 = orb_detector.detectAndCompute(ref_image, None) \n",
    "    kp2, d2 = orb_detector.detectAndCompute(sns_image, None) \n",
    "    return [kp1,kp2,d1,d2]\n",
    "\n",
    "def extract_feature_batch(refs, sns):\n",
    "    output = [[],[],[],[]]\n",
    "    for i in range(refs.shape[0]):\n",
    "        out = extract_features(refs[i], sns[i])\n",
    "        for j in range(4):\n",
    "            output[j].append(out[j])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def batch_match(d1s, d2s):\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    matches = []\n",
    "\n",
    "    for i in range(len(d1s)):\n",
    "        matches.append(matcher.match(d1s[i], d2s[i]))\n",
    "    return matches\n",
    "\n",
    "def validate_match(matches):\n",
    "    for i in range(len(matches)):\n",
    "        matches[i].sort(key = lambda x: x.distance)\n",
    "        matches[i] = matches[i][:int(len(matches)*60)]\n",
    "    return matches\n",
    "\n",
    "def calc_homographies(kp1s, kp2s, matches):\n",
    "    # Define empty matrices of shape no_of_matches * 2. \n",
    "    homographies = []\n",
    "    \n",
    "    temp = matches\n",
    "    matches = []\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i] != []:\n",
    "            matches.append(list(filter(None, temp[i])))\n",
    "            \n",
    "    matches = validate_match(matches)\n",
    "    \n",
    "    for i in range(len(matches)):\n",
    "        p1 = np.zeros((len(matches[i]), 2)) \n",
    "        p2 = np.zeros((len(matches[i]), 2)) \n",
    "        for j in range(len(matches[i])):\n",
    "            p1[j, :] = kp1s[i][matches[i][j].queryIdx].pt \n",
    "            p2[j, :] = kp2s[i][matches[i][j].trainIdx].pt \n",
    "        homography, _ = cv2.findHomography(p1, p2, cv2.RANSAC) \n",
    "        homographies.append(homography)\n",
    "    return homographies\n",
    "\n",
    "def register_images(sns_imgs, homographies, img_size = (IMG_WIDTH,IMG_HEIGHT), save = False):\n",
    "    # Use this matrix to transform the \n",
    "    # colored image wrt the reference image. \n",
    "    transformed_imgs = []\n",
    "    for i in range(sns_imgs.shape[0]):\n",
    "        transformed_img = cv2.warpPerspective(sns_imgs[i], \n",
    "                            homographies[i], img_size) \n",
    "        if save: \n",
    "            cv2.imwrite('aligned_{}.jpg'.format(i), transformed_img) \n",
    "        \n",
    "        transformed_imgs.append(transformed_img)\n",
    "    return transformed_imgs\n",
    "\n",
    "def visualize_matches(ref_imgs, sns_imgs, kp1s, kp2s, matches):\n",
    "    for i in range(ref_imgs.shape[0]):\n",
    "        imMatches = cv2.drawMatches(ref_imgs[i], kp1s[i], sns_imgs[i], kp2s[i], matches[i], None)\n",
    "        cv2.imwrite(\"matches_{}.jpg\".format(i), imMatches)\n",
    "    \n",
    "    \n",
    "def get_alignment_matrix(kprs, kpss, drs, dss):\n",
    "    '''\n",
    "    Inputs\n",
    "        kprs: F keypoints for each reference(query) image of shape N*F*3 with X,Y,Size\n",
    "        kpss: F keypoints for each sensed(train) image of shape N*F*3 with X,Y,Size\n",
    "        drs: F feature descriptors for each reference(query) image of shape N*F*32 \n",
    "        dss: F feature descriptors for each sensed(train) image of shape N*F*32 \n",
    "    Output\n",
    "        aligned feature points and correlated distance of size N*f*7 X1,Y1,Size1, X2, Y2, Size2, Distance\n",
    "    '''\n",
    "\n",
    "    alignment_matrices = None\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    for i in range(kprs.shape[0]):\n",
    "        results = matcher.match(d1s[i], d2s[i])\n",
    "        aligned = []\n",
    "        for r in results:\n",
    "            if len(aligned) >= NUM_MATCHES: break\n",
    "            temp = np.concatenate((kprs[i][r.queryIdx], kpss[i][r.trainIdx]))\n",
    "            aligned.append(np.concatenate((temp, [r.distance])))\n",
    "            \n",
    "        while len(aligned) < NUM_MATCHES:\n",
    "            aligned.append([0,0,0,0,0,0,0])\n",
    "            \n",
    "        aligned = np.array([aligned])\n",
    "        if alignment_matrices is None:\n",
    "            alignment_matrices = aligned\n",
    "        else:\n",
    "            alignment_matrices = np.vstack((alignment_matrices, aligned))\n",
    "            \n",
    "    #sample alignment_matrix\n",
    "    alignment_matrices = np.rint(alignment_matrices).astype(np.uint32)\n",
    "    return alignment_matrices\n",
    "    \n",
    "    \n",
    "def test_cv2_batch(dl):\n",
    "    refs, sns = dl.load_image()\n",
    "    kp1s, kp2s, d1s,d2s = extract_feature_batch(refs,sns)\n",
    "    matches = batch_match(d1s, d2s)\n",
    "    \n",
    "    visualize_matches(refs, sns, kp1s, kp2s, matches)\n",
    "    homos = calc_homographies(kp1s, kp2s, matches)\n",
    "    imgs = register_images(sns, homos)\n",
    "    for i in range(6):\n",
    "            cv2.imwrite(\"registered_cv2{}.jpg\".format(i), imgs[i])\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "NUM_F_POINTS = 2000\n",
    "print(tf.executing_eagerly())\n",
    "def extract_feature_coordinates(ref_image, sns_image):\n",
    "    # Create ORB detector with 5000 features. \n",
    "\n",
    "    ref_image = cv2.cvtColor(ref_image, cv2.COLOR_BGR2GRAY) \n",
    "    sns_image = cv2.cvtColor(sns_image, cv2.COLOR_BGR2GRAY) \n",
    "    orb_detector = cv2.ORB_create(NUM_F_POINTS) \n",
    "    kp1, d1 = orb_detector.detectAndCompute(ref_image, None) \n",
    "    kp2, d2 = orb_detector.detectAndCompute(sns_image, None) \n",
    "    kp1_np, kp2_np = [], []\n",
    "    for i in range(NUM_F_POINTS):\n",
    "        if i < len(kp1):\n",
    "            kp1_np.append([kp1[i].pt[0],kp1[i].pt[1], kp1[i].size ] )\n",
    "        else:\n",
    "            kp1_np.append([0,0,0])\n",
    "            d1 = np.vstack((d1, [np.zeros(32, dtype = np.uint8 )]))\n",
    "            \n",
    "        if i < len(kp2):\n",
    "            kp2_np.append([kp2[i].pt[0],kp2[i].pt[1], kp2[i].size ] )\n",
    "        else:\n",
    "            kp2_np.append([0,0,0])\n",
    "            d2 = np.vstack((d2, [np.zeros(32,dtype = np.uint8 )]))\n",
    "        \n",
    "    kp1_np, kp2_np = np.array(kp1_np) , np.array(kp2_np)\n",
    "\n",
    "    return [kp1_np, kp2_np, d1, d2]\n",
    "\n",
    "def extract_feature_coor_batch(refs, sns):\n",
    "    output = []\n",
    "    for i in range(refs.shape[0]):\n",
    "        out = extract_feature_coordinates(refs[i], sns[i])\n",
    "        for j in range(4):\n",
    "            if len(output) < 4:\n",
    "                output.append(np.expand_dims(out[j], axis=0))\n",
    "            else: \n",
    "                output[j] = np.vstack( (output[j],np.expand_dims(out[j], axis=0)) )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def extract_match_patches(ref_imgs, sns_imgs, kprs, kpss, drs, dss):\n",
    "    '''\n",
    "    output: N * NUM_MATCHES * 2 * PATCH_H * PATCH_W * CHN Example:(6, 500, 2, 6, 6, 3)\n",
    "    '''\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    patches, matches = [], []\n",
    "    for i in range(kprs.shape[0]):\n",
    "        results = matcher.match(drs[i], dss[i])\n",
    "        patch = []\n",
    "        match = []\n",
    "        for r in results:\n",
    "            pair = []\n",
    "            match_p = []\n",
    "            if len(patch) >= NUM_MATCHES: break\n",
    "            x, y, dis = kprs[i][r.queryIdx].astype(np.uint32)\n",
    "            if  x-(BBOX_LENGTH-1) > 0 and y-(BBOX_LENGTH-1) > 0 \\\n",
    "                and x+(BBOX_LENGTH-1)/2 < IMG_WIDTH and y+(BBOX_LENGTH-1)/2 < IMG_HEIGHT:\n",
    "                pair.append(ref_imgs[i][y-int((BBOX_LENGTH-1)/2):y+int((BBOX_LENGTH-1)/2)\\\n",
    "                                       ,x-int((BBOX_LENGTH-1)/2):x+int((BBOX_LENGTH-1)/2)])\n",
    "                \n",
    "            x, y, dis = kpss[i][r.trainIdx].astype(np.uint32)\n",
    "            if  x-(BBOX_LENGTH-1) > 0 and y-(BBOX_LENGTH-1) > 0 \\\n",
    "                and x+(BBOX_LENGTH-1)/2 < IMG_WIDTH and y+(BBOX_LENGTH-1)/2 < IMG_HEIGHT:\n",
    "                pair.append(sns_imgs[i][y-int((BBOX_LENGTH-1)/2):y+int((BBOX_LENGTH-1)/2)\\\n",
    "                                       ,x-int((BBOX_LENGTH-1)/2):x+int((BBOX_LENGTH-1)/2)])\n",
    "                \n",
    "            if len(pair) == 2:\n",
    "                patch.append(np.array(pair))\n",
    "                match.append(r)\n",
    "                \n",
    "        while len(patch) < NUM_MATCHES:\n",
    "            patch.append(np.zeros((2,BBOX_LENGTH-1,BBOX_LENGTH-1, IMG_CHN)))\n",
    "            match.append(None)\n",
    "            \n",
    "        patch = np.array(patch)\n",
    "        patches.append(patch)\n",
    "        match = np.array(match)\n",
    "        matches.append(match)\n",
    "\n",
    "    patches = np.array(patches)\n",
    "    matches = np.array(matches)   \n",
    "\n",
    "    return [patches[:,:,0,:,:,:], patches[:,:,1,:,:,:], matches]\n",
    "    \n",
    "\n",
    "def get_match_info(refs,sns):\n",
    "    \"\"\"\n",
    "    returns in 255 scale\n",
    "    \"\"\"\n",
    "    kprs,kpss, drs, dss = extract_feature_coor_batch(refs,sns)\n",
    "    p_ref, p_sns, matches =  extract_match_patches(refs, sns, kprs, kpss, drs, dss)\n",
    "    return [p_ref, p_sns, matches, kprs, kpss]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.executing_eagerly())\n",
    "def generate_generator_multiple(generator, path, batch_size = 16, img_height = IMG_HEIGHT, img_width = IMG_WIDTH):\n",
    "\n",
    "        gen_ref = generator.flow_from_directory(path,\n",
    "                                              classes = [\"ref\"],\n",
    "                                              target_size = (img_height,img_width),\n",
    "                                              batch_size = batch_size,\n",
    "                                              shuffle=False, \n",
    "                                              seed=7)\n",
    "\n",
    "        gen_sns = generator.flow_from_directory(path,\n",
    "                                              classes = [\"sns\"],\n",
    "                                              target_size = (img_height,img_width),\n",
    "                                              batch_size = batch_size,\n",
    "                                              shuffle=False, \n",
    "                                              seed=7)\n",
    "        while True:\n",
    "                X1i = gen_ref.next()\n",
    "                X2i = gen_sns.next()\n",
    "                x,y,matches, _, _ = get_model_inputs(X1i[0].astype(np.uint8), X2i[0].astype(np.uint8))\n",
    "                \n",
    "                yield [ X1i[0].astype(np.uint8), X2i[0].astype(np.uint8) ], None #Yield both images and their mutual label\n",
    "                \n",
    "class Dataloader:\n",
    "    def __init__(self, train_path, test_path, batch_size = 16):\n",
    "        \n",
    "        train_imgen = keras.preprocessing.image.ImageDataGenerator()\n",
    "        test_imgen = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "        self.train_generator = generate_generator_multiple(generator=train_imgen,\n",
    "                                               path = str(train_path),\n",
    "                                               batch_size=batch_size)       \n",
    "\n",
    "        self.test_generator = generate_generator_multiple(test_imgen,\n",
    "                                              path = str(test_path),\n",
    "                                              batch_size=batch_size)              \n",
    "\n",
    "        \n",
    "    def load_image(self, test = False):\n",
    "        if test:\n",
    "            return next(self.test_generator)[0]\n",
    "        else:\n",
    "            return next(self.train_generator)[0]\n",
    "    \n",
    "    def load_dl(self):\n",
    "        return [self.train_generator, self.test_generator]\n",
    "    \n",
    "    \n",
    "def get_model_inputs(refs,sns):\n",
    "    p_ref, p_sns, matches, kprs, kpss = get_match_info(refs, sns)\n",
    "    p_ref = (p_ref.astype(np.float32) / 255.0 ).reshape((p_ref.shape[0]*p_ref.shape[1],\\\n",
    "                                                        p_ref.shape[2], p_ref.shape[3], p_ref.shape[4]))\n",
    "    p_sns = (p_sns.astype(np.float32) / 255.0).reshape((p_sns.shape[0]*p_sns.shape[1],\\\n",
    "                                                        p_sns.shape[2], p_sns.shape[3], p_sns.shape[4]))\n",
    "    #matches = matches.reshape(matches.shape[0] * matches.shape[1])\n",
    "    return [p_ref, p_sns, matches, kprs, kpss]\n",
    "        \n",
    "def visualize_corresponding_patches(p1, p2):\n",
    "    for j in range(50):\n",
    "        vis = (np.concatenate((p1[0][j], p2[0][j]), axis=1))\n",
    "        cv2.imwrite(\"patch pair {}.jpg\".format(j), vis)\n",
    "        \n",
    "def visualize_coords(img, c):\n",
    "    for j in range(500):\n",
    "        cv2.circle(img[0], (c[0][j][0], c[0][j][1]) , 1, (0, 0, 255), -1)\n",
    "    cv2.imwrite(\"New feture img.jpg\", img[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "DIST_THRES = 100\n",
    "eps = 0.9\n",
    "\n",
    "def feature_extractor_layers(): # shared layer\n",
    "    feature_extractor_input = keras.Input(shape=( (2*int((BBOX_LENGTH-1)/2))**2*IMG_CHN, ) )\n",
    "    x = layers.Flatten()(feature_extractor_input)\n",
    "    x = layers.Dense(128 ,activation='relu', name = \"fc1\", kernel_initializer= keras.initializers.glorot_normal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(64, activation='relu', name= \"fc2\",kernel_initializer= keras.initializers.glorot_normal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    feature_extractor_output = layers.Dropout(0.2)(x)\n",
    "    feature_extractor = keras.models.Model(feature_extractor_input,feature_extractor_output, name='feature_extractor')\n",
    "    \n",
    "    return feature_extractor\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "def patch_dist(p1,p2):\n",
    "\n",
    "    return np.mean(((p1 - p2)**2)**0.5)\n",
    "\n",
    "def get_central_coor(patch,img):\n",
    "    \n",
    "    W,H = patch.shape[0], patch.shape[1]\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            print(\"{} {}\".format(i,j))\n",
    "            if i+W < img.shape[0] and j+H < img.shape[1] and patch_dist(img[i:i+W, j:j+H, :],patch) < 1:\n",
    "                return (i+W/2, j + H/2)\n",
    "            \n",
    "    print(\"patch does not exist in img.\")\n",
    "    return None\n",
    "\n",
    "class PatchRanker(keras.Model):\n",
    "    def __init__(self, name = \"patch_ranker\", **kwargs):\n",
    "        super(PatchRanker, self).__init__(name=name, **kwargs)\n",
    "        self.feature_extractor = feature_extractor_layers()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        refs, snss = inputs\n",
    "        refs = refs.numpy()\n",
    "        snss = snss.numpy()\n",
    "        x,y,matches, _ ,_ = get_model_inputs(refs,snss)\n",
    "        \n",
    "        with tf.GradientTape() as gtape:\n",
    "            f_x = self.feature_extractor(x)\n",
    "            f_y = self.feature_extractor(y)\n",
    "            f_xy = tf.concat([f_x,f_y],1)\n",
    "            z = layers.Dense(64 ,activation='relu', name = \"fc3\" ,kernel_initializer = keras.initializers.glorot_normal())(f_xy)\n",
    "            z = layers.BatchNormalization()(z)\n",
    "            z = layers.Dropout(0.2)(z)\n",
    "            classified = layers.Dense(1 ,activation='sigmoid',kernel_initializer=keras.initializers.glorot_normal())(z)\n",
    "            pred = tf.reshape(classified, [-1, NUM_MATCHES, 1])\n",
    "            inds = tf.argsort(pred, axis = 1)\n",
    "            mask = tf.cast(tf.where(inds < 50, tf.ones_like(inds), tf.zeros_like(inds)), tf.float32 ) # -1*NUM_MATCHES*1\n",
    "            selected_inds = mask[:,:,0]\n",
    "            selected_matches = np.where( selected_inds.numpy() == 1.0, matches, np.zeros_like(matches)  )\n",
    "            good_matches = []\n",
    "            for i in range(selected_matches.shape[0]):\n",
    "                good_matches.append(selected_matches[i][selected_matches[i] != 0])\n",
    "\n",
    "            kprs, kpss, _, _ =  extract_feature_batch(refs,snss)\n",
    "            homo = calc_homographies(kprs, kpss, good_matches)\n",
    "            imgs = np.array(register_images(snss, homo))\n",
    "\n",
    "            for i in range(imgs.shape[0]):\n",
    "                #tf.summary.image('reg_img{}'.format(i), imgs)\n",
    "                cv2.imwrite(\"registered_cv2{}.jpg\".format(i), imgs[i])\n",
    "            #print(\"registered!!\")\n",
    "            _, _, matches, kp1s, kp2s = get_match_info(refs, imgs)\n",
    "\n",
    "            feature_diss = []\n",
    "            coor_diss = []\n",
    "\n",
    "            for i in range(kp1s.shape[0]):\n",
    "                coor_dis = 0\n",
    "                feature_dis = 0\n",
    "                valid_count = 0\n",
    "                for r in matches[i]:\n",
    "                    if r is None:\n",
    "                        continue\n",
    "                    valid_count+=1\n",
    "                    x1, y1, _ = kp1s[i][r.queryIdx]\n",
    "                    x2, y2, _ = kp1s[i][r.trainIdx]\n",
    "                    coor_dis += ((x1-x2)**2 + (y1-y2)**2)**0.5 #euc dist\n",
    "\n",
    "                    feature_dis += r.distance\n",
    "\n",
    "                coor_diss.append(coor_dis/valid_count)\n",
    "                feature_diss.append(feature_dis/valid_count)\n",
    "\n",
    "            coor_diss = np.array(coor_diss)\n",
    "            feature_diss = np.array(feature_diss)\n",
    "\n",
    "\n",
    "            gt = []\n",
    "            for c_d, f_d in zip(coor_diss, feature_diss):\n",
    "                print(c_d)\n",
    "                print(f_d)\n",
    "                print(\"================\")\n",
    "                if (c_d < DIST_THRES and f_d < DIST_THRES):\n",
    "                    gt.append(tf.ones_like(pred[0]))\n",
    "                else:\n",
    "                    gt.append(tf.zeros_like(pred[0]))\n",
    "\n",
    "            gt = np.array(gt)\n",
    "\n",
    "            loss = tf.reduce_mean(((gt - pred)*mask)**2)\n",
    "            self.add_loss(loss)\n",
    "            \n",
    "        '''    \n",
    "        grads = gtape.gradient(pred, self.trainable_variables)\n",
    "        \n",
    "        for i in range(len(grads)):\n",
    "            print(\"Gradietns:\")\n",
    "            print(grads[i])\n",
    "            print(\"Weights:\")\n",
    "            print(self.trainable_variables[i])\n",
    "            print(\"==========================\")\n",
    "        '''\n",
    "        return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Found 6 images belonging to 1 classes.\n",
      "Found 6 images belonging to 1 classes.\n",
      "WARNING:tensorflow:Output output_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to output_1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222.45649797936554\n",
      "41.77777777777778\n",
      "================\n",
      "109.84208308256167\n",
      "36.54545454545455\n",
      "================\n",
      "304.6134514722081\n",
      "51.4792899408284\n",
      "================\n",
      "276.7068506967201\n",
      "54.3433734939759\n",
      "================\n",
      "317.6988668135456\n",
      "60.582\n",
      "================\n",
      "281.4445279196394\n",
      "57.42948717948718\n",
      "================\n",
      "WARNING:tensorflow:From /home/charles/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Found 6 images belonging to 1 classes.\n",
      "Found 6 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.23850707610103\n",
      "44.43795620437956\n",
      "================\n",
      "77.03407833909479\n",
      "35.91304347826087\n",
      "================\n",
      "297.3456376018237\n",
      "52.914081145584724\n",
      "================\n",
      "266.85955041443805\n",
      "52.66983372921615\n",
      "================\n",
      "328.1236328224745\n",
      "60.74364896073903\n",
      "================\n",
      "294.5862820072629\n",
      "51.82636655948553\n",
      "================\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0307 - val_loss: 0.0158\n",
      "Epoch 2/100\n",
      "167.69585360952283\n",
      "32.69444444444444\n",
      "================\n",
      "93.58386409251666\n",
      "43.0\n",
      "================\n",
      "277.56667837054863\n",
      "48.84931506849315\n",
      "================\n",
      "269.47161727981745\n",
      "53.956011730205276\n",
      "================\n",
      "314.01717651208526\n",
      "54.497757847533634\n",
      "================\n",
      "299.405832909755\n",
      "55.0593471810089\n",
      "================\n",
      "Found 6 images belonging to 1 classes.\n",
      "Found 6 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291.8882025800146\n",
      "46.994594594594595\n",
      "================\n",
      "112.32879206231958\n",
      "37.833333333333336\n",
      "================\n",
      "297.4885342106448\n",
      "48.78274760383387\n",
      "================\n",
      "254.90474227061839\n",
      "55.630057803468205\n",
      "================\n",
      "340.71356832181664\n",
      "64.48101265822785\n",
      "================\n",
      "303.16913348568835\n",
      "51.02709359605911\n",
      "================\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.0322 - val_loss: 0.0018\n",
      "Epoch 3/100\n",
      "226.20350904625957\n",
      "47.661654135338345\n",
      "================\n",
      "230.1785666539722\n",
      "43.40983606557377\n",
      "================\n",
      "280.87798478738796\n",
      "55.13370473537604\n",
      "================\n",
      "261.2520218076672\n",
      "56.07037037037037\n",
      "================\n",
      "309.5282839785599\n",
      "56.13937282229965\n",
      "================\n",
      "280.2140296397142\n",
      "56.75068493150685\n",
      "================\n",
      "Found 6 images belonging to 1 classes.\n",
      "Found 6 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 828, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"<ipython-input-7-bb0bebf3be19>\", line 19, in generate_generator_multiple\n",
      "    X2i = gen_sns.next()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
      "    interpolation=self.interpolation)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
      "    img = img.resize(width_height_tuple, resample)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/Image.py\", line 1817, in resize\n",
      "    self.load()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/ImageFile.py\", line 242, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "Process Keras_worker_ForkPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 828, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"<ipython-input-7-bb0bebf3be19>\", line 19, in generate_generator_multiple\n",
      "    X2i = gen_sns.next()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
      "    interpolation=self.interpolation)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
      "    img = img.resize(width_height_tuple, resample)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/Image.py\", line 1817, in resize\n",
      "    self.load()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/ImageFile.py\", line 242, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "Process Keras_worker_ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Process Keras_worker_ForkPoolWorker-2:\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 828, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"<ipython-input-7-bb0bebf3be19>\", line 19, in generate_generator_multiple\n",
      "    X2i = gen_sns.next()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
      "    interpolation=self.interpolation)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
      "    img = img.resize(width_height_tuple, resample)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/Image.py\", line 1817, in resize\n",
      "    self.load()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/ImageFile.py\", line 242, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 828, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"<ipython-input-7-bb0bebf3be19>\", line 19, in generate_generator_multiple\n",
      "    X2i = gen_sns.next()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
      "    interpolation=self.interpolation)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
      "    img = img.resize(width_height_tuple, resample)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/Image.py\", line 1817, in resize\n",
      "    self.load()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/PIL/ImageFile.py\", line 242, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images belonging to 1 classes.\n",
      "Found 6 images belonging to 1 classes.\n",
      "Found 6 images belonging to 1 classes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6b546c0c79e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                 \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestset_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                 \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                 shuffle=False)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#create DNN model\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 6\n",
    "trainset_size = 6\n",
    "testset_size = 6\n",
    "epochs = 100\n",
    "\n",
    "patch_ranker = PatchRanker(dynamic=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "patch_ranker.compile(optimizer = optimizer,run_eagerly = True)\n",
    "\n",
    "dl = Dataloader(train_path = \"./data/train\", test_path= \"./data/test\", batch_size = batch_size)\n",
    "train_generator, test_generator = dl.load_dl()\n",
    "\n",
    "#print(patch_ranker)\n",
    "\n",
    "patch_ranker.fit_generator(train_generator,\n",
    "                                steps_per_epoch=trainset_size/batch_size,\n",
    "                                epochs = epochs,\n",
    "                                validation_data = test_generator,\n",
    "                                validation_steps = testset_size/batch_size,\n",
    "                                use_multiprocessing = True,\n",
    "                                shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
